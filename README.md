# DSO Exam - GitOps Microservices Platform

A complete Event-Driven Microservices architecture deployed on Kubernetes (Kind) using GitOps principles with ArgoCD, Strimzi Kafka, and NGINX Ingress.

## ðŸ— Architecture

The platform consists of three main application components and a robust infrastructure layer:

* **Frontend (Next.js):** Public-facing UI for submitting messages.
* **Backend (Go):** API that produces events to Kafka.
* **Consumer (Go):** Worker that consumes events and persists data to PostgreSQL.
* **Infrastructure:** PostgreSQL, Kafka (Strimzi Operator), NGINX Ingress, and Ops Tools.

### Data Flow
`User` -> `Ingress` -> `Frontend` -> `Backend` -> `Kafka Topic` -> `Consumer` -> `PostgreSQL`

---

## ðŸ›  Prerequisites

Ensure you have the following tools installed:
* [Docker](https://www.docker.com/)
* [Kind](https://kind.sigs.k8s.io/) (Kubernetes in Docker)
* [Kubectl](https://kubernetes.io/docs/tasks/tools/)
* [Helm](https://helm.sh/)

---

## ðŸš€ Quick Start Guide

Follow these steps to spin up the entire environment from scratch.

### 1. Initialize the Cluster
Create the Kind cluster with the required port mappings (80/443 for Ingress).
Config file location : `kind/kind-config.yaml`

```bash
kind create cluster --config kind-config.yaml
```

Optional : export kubeconfig for using Len application

```bash
kind export kubeconfig --name devsecops-lab
```

### 2. Install ArgoCD
Deploy the GitOps engine.  
Config file location : `argocd/argocd-values.yaml`

```bash
# Add the ArgoCD Helm repo
helm repo add argo [https://argoproj.github.io/argo-helm](https://argoproj.github.io/argo-helm)
helm repo update

# Install ArgoCD
helm install argocd argo/argo-cd \
  --namespace argocd \
  --create-namespace \
  --version 9.1.9 \
  --values argocd-values.yaml
```

### 3. Deploy Argo Application "control-center"
Argo application name `control-center` design for bootstrap infrastructure and manage application in a single kustomize.  
Config file location : `argocd/control-center/values.yaml`

```bash
kubectl apply -f values.yaml
```

---

## ðŸŒ Access Points

Once all pods are running, access the services locally via your browser.

| Service | URL | Credentials (If any) |
| :--- | :--- | :--- |
| **Main App** | [http://message.localhost](http://message.localhost) | - |
| **ArgoCD** | [http://argocd.localhost](http://argocd.localhost) | `admin` / *(Get password via CLI)* |
| **Kafka UI** | [http://kafka.localhost](http://kafka.localhost) | - |
| **PgAdmin** | [http://pgadmin.localhost](http://pgadmin.localhost) | `admin@admin.com` / `admin` |

> **Note:** If `*.localhost` domains do not resolve, add `127.0.0.1 message.localhost argocd.localhost` to your `/etc/hosts` file.

---

## ðŸ“‚ Repository Structure

The repository follows a strict separation of concerns for GitOps, isolating infrastructure from application logic.
this example can be separate to multiple repository but for easy searching I merge it into one.
location path : `gitops/control-center`

```text
/
â”œâ”€â”€ base/                               # Base application that need to have on every cluster
â”‚   â”œâ”€â”€ app/                            # Base application list
â”‚   â”‚  â”œâ”€â”€ ingress-nginx.yaml           # App: Nginx Ingress
â”‚   â”‚  â””â”€â”€ strimzi-operator.yaml        # App: strimzi-operator
â”‚   â”œâ”€â”€ namespace/                      # Base namespace list
â”‚   â”‚  â”œâ”€â”€ backend.yaml                 # Namespace of backend
â”‚   â”‚  â”œâ”€â”€ consumer.yaml                # Namespace of consumer
â”‚   â”‚  â”œâ”€â”€ frontend.yaml                # Namespace of frontend
â”‚   â”‚  â””â”€â”€ postgres.yaml                # Namespace of postgres
â”‚   â””â”€â”€ kustomization.yaml
â””â”€â”€ overlay/                            # Overlay on each cluster
    â””â”€â”€ lab-dev-th/                     # Can separate into multiple clusters, environments, regions
       â”œâ”€â”€ app/                         # Application list (in the future each directory can be repository)
       â”‚  â”œâ”€â”€ backend/                  # App: backend
       â”‚  â”‚  â”œâ”€â”€ deployment.yaml
       â”‚  â”‚  â”œâ”€â”€ ingress.yaml
       â”‚  â”‚  â”œâ”€â”€ kustomization.yaml     # Manage apps, configs, secrets
       â”‚  â”‚  â””â”€â”€ service.yaml
       â”‚  â”œâ”€â”€ consumer/
       â”‚  â”‚  â”œâ”€â”€ deployment.yaml        # App: consumer
       â”‚  â”‚  â”œâ”€â”€ init-job.yaml          # Job: initial database
       â”‚  â”‚  â”œâ”€â”€ kustomization.yaml     # Manage apps, configs, secrets
       â”‚  â””â”€â”€ frontend/                 # App: frontend
       â”‚     â”œâ”€â”€ deployment.yaml
       â”‚     â”œâ”€â”€ ingress.yaml
       â”‚     â”œâ”€â”€ kustomization.yaml     # Manage apps
       â”‚     â””â”€â”€ service.yaml
       â”œâ”€â”€ argocd/
       â”‚  â””â”€â”€ ingress.yaml              # Ingress for argoCD
       â”œâ”€â”€ kafka/                       # App: kafka
       â”‚  â”œâ”€â”€ kafka-cluster.yaml
       â”‚  â”œâ”€â”€ kafka-nodepool.yaml
       â”‚  â”œâ”€â”€ kafka-ui.yaml             # App: kafka-ui
       â”‚  â””â”€â”€ topics.yaml
       â”œâ”€â”€ postgres/
       â”‚  â”œâ”€â”€ pgadmin.yaml              # App: pgadmin
       â”‚  â””â”€â”€ postgres.yaml             # App: postgres
       â””â”€â”€ kustomization.yaml           # Main kustomize manage cluster
```

---

## ðŸ”§ Operational Cheatsheet

Useful commands for verifying and debugging the environment.

**1. Get ArgoCD Admin Password**
```bash
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo
```

**2.Check Kafka Broker Status**
```bash
kubectl get kafka -n kafka
```

**3. Verify Postgres Data Persistence Check if the messages are actually saved in the database:**
```bash
kubectl exec -it -n postgres deploy/postgres -- psql -U postgres -d messages -c "SELECT * FROM messages;"
```

**4. Debugging Pod Logs If something isn't working, check the logs for specific services:**
```bash
# Backend Logs
kubectl logs -n backend -l app=backend --tail=20

# Consumer Logs
kubectl logs -n consumer -l app=consumer --tail=20
```

---

## ðŸ”® Future Improvements

To elevate this platform to a production-grade enterprise standard, the following enhancements are proposed:

### 1. Advanced Security with HashiCorp Vault & External Secrets
**Current State:** Secrets (database credentials, API keys) are currently managed via Kustomize `secretGenerator` or Kubernetes native Secrets. While functional for development, this exposes secrets in the cluster state or requires manual management.

**Proposed Solution:** Implement a **"Secret Zero"** architecture using:
* **HashiCorp Vault:** To serve as the centralized, encrypted source of truth for all secrets.
* **External Secrets Operator (ESO):** To automatically fetch secrets from Vault and inject them into Kubernetes Native Secrets.

**Benefits:**
* **Rotation:** Automated rotation of database passwords without restarting pods.
* **Audit:** Centralized logging of who accessed which secret and when.
* **GitOps Compliance:** No sensitive data ever touches the Git repository, not even in encrypted formats (like Sealed Secrets).

### 2. Infrastructure as Code (IaC) with Terraform
**Current State:** The cluster bootstrapping (Kind creation) and initial tooling installation (ArgoCD via Helm) are currently performed using imperative shell scripts. This is prone to human error and difficult to replicate exactly across different environments (Dev, Staging, Prod).

**Proposed Solution:** adopt **Terraform** or **OpenTofu** to provision the entire bootstrapping layer.
* Use the **Kind Provider** to spin up the cluster configuration.
* Use the **Helm Provider** to install the initial ArgoCD instance in a declarative manner.

**Benefits:**
* **State Management:** Terraform keeps track of what is installed. If a configuration changes, Terraform only applies the "Delta" (difference) rather than re-running the whole script.
* **Idempotency:** Running the script 100 times results in the same stable state, preventing "configuration drift."
* **Disaster Recovery:** The entire cluster infrastructure can be destroyed and recreated with a single command (`terraform apply`).